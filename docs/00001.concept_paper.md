Excellent idea! Docker will significantly improve deployment, dependency management, and portability. Let me integrate Docker into the design with a production-ready setup for Windows 11 + Docker Desktop.

# Enhanced Design with Docker Integration

## Docker Architecture Overview

We'll use a **multi-container setup** with Docker Compose for clean separation of concerns and easy management:

```
┌─────────────────────────────────────────────────────────┐
│                    Docker Compose                        │
├──────────────┬──────────────┬──────────────┬────────────┤
│   Backend    │   Frontend   │   LanceDB    │  GPU Utils │
│   (FastAPI)  │   (React)    │   (Volume)   │  (llama.cpp)│
│   Port 8000  │   Port 3000  │   Persistent │  GPU Access│
└──────────────┴──────────────┴──────────────┴────────────┘
         │              │              │              │
         └──────────────┴──────────────┴──────────────┘
                         │
                    Host Network
                         │
                   Knowledge Base
                   (Mounted Volume)
```

## Complete Docker Setup

### Project Structure

```
neural-knowledge-base/
├── docker-compose.yml
├── .env
├── .dockerignore
├── backend/
│   ├── Dockerfile
│   ├── requirements.txt
│   ├── app/
│   │   ├── main.py
│   │   ├── ingestion.py
│   │   ├── indexing.py
│   │   ├── retrieval.py
│   │   └── generation.py
│   └── models/          # GGUF models (mounted)
├── frontend/
│   ├── Dockerfile
│   ├── package.json
│   └── src/
├── data/
│   ├── knowledge_base/  # Your documents (mounted)
│   ├── lancedb/         # Database (volume)
│   └── cache/           # Ingestion cache (volume)
└── scripts/
    ├── download_models.sh
    └── init_db.sh
```

### 1. Docker Compose Configuration

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Backend API Service
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: kb-backend
    ports:
      - "8000:8000"
    volumes:
      # Mount knowledge base (read your documents)
      - ./data/knowledge_base:/app/knowledge_base:ro
      # Persistent database
      - lancedb-data:/app/lancedb
      # Cache for faster re-ingestion
      - ingestion-cache:/app/cache
      # Models directory (download once, reuse)
      - ./backend/models:/app/models:ro
      # Config
      - ./backend/app:/app/app:ro
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_PATH=/app/models/mistral-nemo-12b.Q5_K_M.gguf
      - EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
      - LANCEDB_PATH=/app/lancedb
      - KNOWLEDGE_BASE_PATH=/app/knowledge_base
      - CACHE_PATH=/app/cache
      - LOG_LEVEL=INFO
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - kb-network

  # Frontend Service
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: kb-frontend
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_WS_URL=ws://localhost:8000
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - kb-network

  # Optional: Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    container_name: kb-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - backend
      - frontend
    restart: unless-stopped
    networks:
      - kb-network
    profiles:
      - production

volumes:
  lancedb-data:
    driver: local
  ingestion-cache:
    driver: local

networks:
  kb-network:
    driver: bridge
```

### 2. Backend Dockerfile (GPU-Optimized)

```dockerfile
# backend/Dockerfile
FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    curl \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Install llama-cpp-python with CUDA support
RUN CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip3 install llama-cpp-python --force-reinstall --no-cache-dir

# Download spaCy model
RUN python3 -m spacy download en_core_web_sm

# Copy application code
COPY app/ /app/app/

# Create necessary directories
RUN mkdir -p /app/knowledge_base /app/lancedb /app/cache /app/models

# Expose API port
EXPOSE 8000

# Health check endpoint
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run FastAPI with uvicorn
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
```

### 3. Backend Requirements

```txt
# backend/requirements.txt
fastapi==0.109.0
uvicorn[standard]==0.27.0
websockets==12.0
pydantic==2.5.3
python-multipart==0.0.6

# AI/ML Stack
sentence-transformers==2.3.1
lancedb==0.5.0
llama-cpp-python==0.2.27
torch==2.1.2
transformers==4.37.0

# Document Processing
unstructured[all-docs]==0.12.0
pypdf==4.0.0
python-docx==1.1.0
pyyaml==6.0.1
spacy==3.7.2

# Utilities
loguru==0.7.2
watchdog==3.0.0
aiofiles==23.2.1
python-dotenv==1.0.0
redis==5.0.1  # Optional: for caching
```

### 4. Frontend Dockerfile

```dockerfile
# frontend/Dockerfile
FROM node:20-alpine AS builder

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm ci

# Copy source code
COPY . .

# Build production bundle
RUN npm run build

# Production stage
FROM nginx:alpine

# Copy built assets
COPY --from=builder /app/build /usr/share/nginx/html

# Copy nginx configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 3000

CMD ["nginx", "-g", "daemon off;"]
```

### 5. Environment Configuration

```bash
# .env
# Model Configuration
MODEL_PATH=./backend/models/mistral-nemo-12b.Q5_K_M.gguf
EMBEDDING_MODEL=BAAI/bge-large-en-v1.5

# Paths
KNOWLEDGE_BASE_PATH=./data/knowledge_base
LANCEDB_PATH=./data/lancedb
CACHE_PATH=./data/cache

# Performance
MAX_WORKERS=4
CUDA_VISIBLE_DEVICES=0
GPU_MEMORY_FRACTION=0.8

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=http://localhost:3000

# Search Configuration
DEFAULT_TOP_K=5
TITLE_BOOST=1.0
TAG_BOOST=0.7
SEMANTIC_BOOST=0.5

# LLM Configuration
CONTEXT_LENGTH=8192
MAX_TOKENS=512
TEMPERATURE=0.3
```

### 6. Enhanced Backend Main Application

```python
# backend/app/main.py
from fastapi import FastAPI, WebSocket, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Optional
import os
from loguru import logger

from .ingestion import EnhancedIngestion
from .indexing import KnowledgeIndex
from .generation import RAGGenerator

# Initialize FastAPI
app = FastAPI(
    title="Neural Knowledge Base API",
    version="2.0.0",
    description="Privacy-first RAG system with weighted hierarchical search"
)

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("CORS_ORIGINS", "http://localhost:3000").split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global instances (initialized on startup)
ingestor: Optional[EnhancedIngestion] = None
index: Optional[KnowledgeIndex] = None
generator: Optional[RAGGenerator] = None

# Models
class Query(BaseModel):
    text: str
    top_k: int = 5
    filters: Dict = {}

class IngestRequest(BaseModel):
    force: bool = False

class SearchResponse(BaseModel):
    results: List[Dict]
    query: str
    execution_time: float

@app.on_event("startup")
async def startup_event():
    """Initialize components on startup"""
    global ingestor, index, generator
    
    logger.info("Initializing Neural Knowledge Base...")
    
    # Initialize ingestion
    ingestor = EnhancedIngestion(
        input_dir=os.getenv("KNOWLEDGE_BASE_PATH", "./knowledge_base"),
        cache_file=os.path.join(os.getenv("CACHE_PATH", "./cache"), "ingestion_cache.json")
    )
    
    # Initialize index
    index = KnowledgeIndex(
        db_path=os.getenv("LANCEDB_PATH", "./lancedb")
    )
    
    # Initialize generator
    generator = RAGGenerator(
        model_path=os.getenv("MODEL_PATH", "./models/mistral-nemo-12b.Q5_K_M.gguf")
    )
    
    logger.info("Startup complete!")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "gpu_available": generator.llm is not None if generator else False,
        "index_ready": index is not None,
        "documents": index.get_document_count() if index else 0
    }

@app.get("/stats")
async def get_stats():
    """Get system statistics"""
    if not index:
        raise HTTPException(status_code=503, detail="Index not initialized")
    
    return {
        "total_documents": index.get_document_count(),
        "total_chunks": index.get_chunk_count(),
        "unique_sources": index.get_unique_sources(),
        "cache_stats": ingestor.get_cache_stats() if ingestor else {}
    }

@app.post("/search", response_model=SearchResponse)
async def search(query: Query):
    """Search knowledge base"""
    if not index:
        raise HTTPException(status_code=503, detail="Index not initialized")
    
    import time
    start_time = time.time()
    
    results = index.weighted_search(
        query=query.text,
        top_k=query.top_k
    )
    
    execution_time = time.time() - start_time
    
    return SearchResponse(
        results=results,
        query=query.text,
        execution_time=execution_time
    )

@app.websocket("/ws/chat")
async def chat_websocket(websocket: WebSocket):
    """WebSocket endpoint for streaming chat"""
    await websocket.accept()
    
    try:
        while True:
            # Receive query
            data = await websocket.receive_json()
            query_text = data.get("query", "")
            
            if not query_text:
                continue
            
            # Retrieve contexts
            contexts = index.weighted_search(query_text, top_k=5)
            
            # Send context metadata first
            await websocket.send_json({
                "type": "contexts",
                "data": [
                    {
                        "title": c["title"],
                        "source": c["source"],
                        "match_type": c["match_type"],
                        "score": c["score"]
                    }
                    for c in contexts
                ]
            })
            
            # Stream answer
            async for chunk in generator.generate_answer_async(query_text, contexts):
                await websocket.send_json({
                    "type": "chunk",
                    "data": chunk
                })
            
            # Send completion signal
            await websocket.send_json({
                "type": "complete"
            })
            
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        await websocket.close()

@app.post("/ingest")
async def trigger_ingestion(
    request: IngestRequest,
    background_tasks: BackgroundTasks
):
    """Trigger document ingestion and indexing"""
    if not ingestor or not index:
        raise HTTPException(status_code=503, detail="Services not initialized")
    
    def run_ingestion():
        logger.info("Starting background ingestion...")
        docs = ingestor.ingest_directory()
        index.create_or_update_table(docs)
        logger.info(f"Ingestion complete: {len(docs)} documents processed")
    
    background_tasks.add_task(run_ingestion)
    
    return {
        "status": "started",
        "message": "Ingestion running in background"
    }

@app.get("/documents")
async def list_documents(skip: int = 0, limit: int = 50):
    """List all indexed documents"""
    if not index:
        raise HTTPException(status_code=503, detail="Index not initialized")
    
    return index.list_documents(skip=skip, limit=limit)

@app.get("/documents/{source}")
async def get_document(source: str):
    """Get specific document with all chunks"""
    if not index:
        raise HTTPException(status_code=503, detail="Index not initialized")
    
    return index.get_document_by_source(source)

@app.delete("/documents/{source}")
async def delete_document(source: str):
    """Delete document from index"""
    if not index:
        raise HTTPException(status_code=503, detail="Index not initialized")
    
    index.delete_document(source)
    return {"status": "deleted", "source": source}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host=os.getenv("API_HOST", "0.0.0.0"),
        port=int(os.getenv("API_PORT", 8000))
    )
```

### 7. Docker Management Scripts

```bash
# scripts/setup.sh
#!/bin/bash
# Initial setup script for Windows (run in Git Bash or WSL)

echo "Setting up Neural Knowledge Base..."

# Create directory structure
mkdir -p data/knowledge_base data/lancedb data/cache backend/models

# Download models (example)
echo "Download models manually to backend/models/"
echo "Recommended: mistral-nemo-12b.Q5_K_M.gguf from HuggingFace"

# Create .env if not exists
if [ ! -f .env ]; then
    cp .env.example .env
    echo ".env created - please configure"
fi

echo "Setup complete! Run 'docker-compose up -d' to start"
```

```powershell
# scripts/setup.ps1
# PowerShell setup script for Windows

Write-Host "Setting up Neural Knowledge Base..." -ForegroundColor Green

# Create directory structure
New-Item -ItemType Directory -Force -Path "data\knowledge_base"
New-Item -ItemType Directory -Force -Path "data\lancedb"
New-Item -ItemType Directory -Force -Path "data\cache"
New-Item -ItemType Directory -Force -Path "backend\models"

# Copy .env template
if (-not (Test-Path ".env")) {
    Copy-Item ".env.example" ".env"
    Write-Host ".env created - please configure" -ForegroundColor Yellow
}

Write-Host @"
Setup complete!

Next steps:
1. Download models to backend/models/
   - Mistral-Nemo: https://huggingface.co/TheBloke/Mistral-Nemo-Instruct-2407-GGUF
2. Add documents to data/knowledge_base/
3. Run: docker-compose up -d

"@ -ForegroundColor Cyan
```

### 8. Quick Start Commands

```bash
# .dockerignore
__pycache__
*.pyc
*.pyo
*.pyd
.Python
*.so
*.egg
*.egg-info
dist
build
.git
.gitignore
.env
*.log
.vscode
.idea
node_modules
*.md
!README.md
```

```bash
# Start services
docker-compose up -d

# View logs
docker-compose logs -f backend

# Rebuild after code changes
docker-compose up -d --build backend

# Trigger ingestion
curl -X POST http://localhost:8000/ingest

# Check health
curl http://localhost:8000/health

# Stop services
docker-compose down

# Complete cleanup (WARNING: deletes data)
docker-compose down -v
```

### 9. Windows-Specific Optimizations

```yaml
# docker-compose.override.yml (for Windows)
version: '3.8'

services:
  backend:
    # Use WSL2 backend for better performance
    volumes:
      # Use named volumes for better performance on Windows
      - type: bind
        source: ${USERPROFILE}/Documents/KnowledgeBase
        target: /app/knowledge_base
        read_only: true
    environment:
      # Windows-specific GPU settings
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
```

### 10. Production Deployment Checklist

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  backend:
    restart: always
    environment:
      - LOG_LEVEL=WARNING
    deploy:
      resources:
        limits:
          memory: 32G
        reservations:
          memory: 16G

  frontend:
    build:
      args:
        - NODE_ENV=production

  nginx:
    profiles: []  # Enable nginx in production
    volumes:
      - ./ssl:/etc/nginx/ssl:ro
```

## Benefits of This Docker Setup

1. **Reproducibility**: Same environment everywhere (dev, staging, prod)
2. **GPU Isolation**: CUDA toolkit in container, no host conflicts
3. **Easy Updates**: `docker-compose pull && docker-compose up -d`
4. **Resource Control**: Set memory/GPU limits per service
5. **Clean Uninstall**: `docker-compose down -v` removes everything
6. **Windows Native**: Works seamlessly with Docker Desktop
7. **Persistent Data**: Volumes survive container recreation
8. **Health Monitoring**: Built-in health checks

## Quick Start for Your Setup

1. **Install Prerequisites**:
   - Docker Desktop with WSL2 backend
   - NVIDIA Container Toolkit (for GPU support)

2. **Clone and Setup**:
   ```bash
   git clone <your-repo>
   cd neural-knowledge-base
   ./scripts/setup.ps1  # PowerShell
   ```

3. **Download Models**:
   - Place GGUF model in `backend/models/`
   - Embeddings auto-download on first run

4. **Add Documents**:
   - Copy files to `data/knowledge_base/`

5. **Launch**:
   ```bash
   docker-compose up -d
   ```

6. **Access**:
   - Frontend: http://localhost:3000
   - API: http://localhost:8000/docs

Would you like me to create the complete React frontend code or add monitoring/logging tools (Prometheus/Grafana) to the Docker setup?